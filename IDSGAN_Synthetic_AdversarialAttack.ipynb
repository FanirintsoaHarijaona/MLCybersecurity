{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\nfrom torch.utils.data import DataLoader, Dataset","metadata":{"execution":{"iopub.status.busy":"2023-08-11T22:53:00.210126Z","iopub.execute_input":"2023-08-11T22:53:00.210915Z","iopub.status.idle":"2023-08-11T22:53:05.108734Z","shell.execute_reply.started":"2023-08-11T22:53:00.210882Z","shell.execute_reply":"2023-08-11T22:53:05.107687Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available(): \n    dev = \"cuda:0\"\nelse:\n    dev = \"cpu\"\n    \ndevice = torch.device(dev)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:10:05.376629Z","iopub.execute_input":"2023-08-10T14:10:05.376988Z","iopub.status.idle":"2023-08-10T14:10:05.386175Z","shell.execute_reply.started":"2023-08-10T14:10:05.376936Z","shell.execute_reply":"2023-08-10T14:10:05.385155Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:10:05.388761Z","iopub.execute_input":"2023-08-10T14:10:05.389103Z","iopub.status.idle":"2023-08-10T14:10:05.396577Z","shell.execute_reply.started":"2023-08-10T14:10:05.389071Z","shell.execute_reply":"2023-08-10T14:10:05.395556Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/UNR-IDD/UNR-IDD.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:10:05.398840Z","iopub.execute_input":"2023-08-10T14:10:05.399227Z","iopub.status.idle":"2023-08-10T14:10:05.566015Z","shell.execute_reply.started":"2023-08-10T14:10:05.399197Z","shell.execute_reply":"2023-08-10T14:10:05.565019Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"df = df.drop(columns = \"Binary Label\")","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:10:05.569039Z","iopub.execute_input":"2023-08-10T14:10:05.569396Z","iopub.status.idle":"2023-08-10T14:10:05.579498Z","shell.execute_reply.started":"2023-08-10T14:10:05.569361Z","shell.execute_reply":"2023-08-10T14:10:05.578617Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:10:05.581352Z","iopub.execute_input":"2023-08-10T14:10:05.581744Z","iopub.status.idle":"2023-08-10T14:10:05.603435Z","shell.execute_reply.started":"2023-08-10T14:10:05.581709Z","shell.execute_reply":"2023-08-10T14:10:05.602107Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"             Switch ID Port Number  Received Packets  Received Bytes  \\\n0  of:000000000000000c     Port#:1               132            9181   \n1  of:000000000000000c     Port#:2               187         6304498   \n2  of:000000000000000c     Port#:3               235         6311567   \n3  of:000000000000000c     Port#:4                59            7878   \n4  of:000000000000000a     Port#:1               188         6304547   \n\n   Sent Bytes  Sent Packets  Port alive Duration (S)  Packets Rx Dropped  \\\n0     6311853           238                       46                   0   \n1       15713           171                       46                   0   \n2        8030            58                       46                   0   \n3       16439           182                       46                   0   \n4       16497           183                       46                   0   \n\n   Packets Tx Dropped  Packets Rx Errors  ...  Unknown Load/Rate  \\\n0                   0                  0  ...                  0   \n1                   0                  0  ...                  0   \n2                   0                  0  ...                  0   \n3                   0                  0  ...                  0   \n4                   0                  0  ...                  0   \n\n   Unknown Load/Latest  Latest bytes counter  is_valid  Table ID  \\\n0                    0                     0      True         0   \n1                    0                     0      True         0   \n2                    0                     0      True         0   \n3                    0                     0      True         0   \n4                    0                     0      True         0   \n\n   Active Flow Entries  Packets Looked Up  Packets Matched  Max Size    Label  \n0                    9                767              688        -1  TCP-SYN  \n1                    9                767              688        -1  TCP-SYN  \n2                    9                767              688        -1  TCP-SYN  \n3                    9                767              688        -1  TCP-SYN  \n4                    7                489              403        -1  TCP-SYN  \n\n[5 rows x 33 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Switch ID</th>\n      <th>Port Number</th>\n      <th>Received Packets</th>\n      <th>Received Bytes</th>\n      <th>Sent Bytes</th>\n      <th>Sent Packets</th>\n      <th>Port alive Duration (S)</th>\n      <th>Packets Rx Dropped</th>\n      <th>Packets Tx Dropped</th>\n      <th>Packets Rx Errors</th>\n      <th>...</th>\n      <th>Unknown Load/Rate</th>\n      <th>Unknown Load/Latest</th>\n      <th>Latest bytes counter</th>\n      <th>is_valid</th>\n      <th>Table ID</th>\n      <th>Active Flow Entries</th>\n      <th>Packets Looked Up</th>\n      <th>Packets Matched</th>\n      <th>Max Size</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>of:000000000000000c</td>\n      <td>Port#:1</td>\n      <td>132</td>\n      <td>9181</td>\n      <td>6311853</td>\n      <td>238</td>\n      <td>46</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>True</td>\n      <td>0</td>\n      <td>9</td>\n      <td>767</td>\n      <td>688</td>\n      <td>-1</td>\n      <td>TCP-SYN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>of:000000000000000c</td>\n      <td>Port#:2</td>\n      <td>187</td>\n      <td>6304498</td>\n      <td>15713</td>\n      <td>171</td>\n      <td>46</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>True</td>\n      <td>0</td>\n      <td>9</td>\n      <td>767</td>\n      <td>688</td>\n      <td>-1</td>\n      <td>TCP-SYN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>of:000000000000000c</td>\n      <td>Port#:3</td>\n      <td>235</td>\n      <td>6311567</td>\n      <td>8030</td>\n      <td>58</td>\n      <td>46</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>True</td>\n      <td>0</td>\n      <td>9</td>\n      <td>767</td>\n      <td>688</td>\n      <td>-1</td>\n      <td>TCP-SYN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>of:000000000000000c</td>\n      <td>Port#:4</td>\n      <td>59</td>\n      <td>7878</td>\n      <td>16439</td>\n      <td>182</td>\n      <td>46</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>True</td>\n      <td>0</td>\n      <td>9</td>\n      <td>767</td>\n      <td>688</td>\n      <td>-1</td>\n      <td>TCP-SYN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>of:000000000000000a</td>\n      <td>Port#:1</td>\n      <td>188</td>\n      <td>6304547</td>\n      <td>16497</td>\n      <td>183</td>\n      <td>46</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>True</td>\n      <td>0</td>\n      <td>7</td>\n      <td>489</td>\n      <td>403</td>\n      <td>-1</td>\n      <td>TCP-SYN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 33 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"len(df.Label.unique())","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:10:05.605281Z","iopub.execute_input":"2023-08-10T14:10:05.605706Z","iopub.status.idle":"2023-08-10T14:10:05.616884Z","shell.execute_reply.started":"2023-08-10T14:10:05.605673Z","shell.execute_reply":"2023-08-10T14:10:05.615662Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"6"},"metadata":{}}]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:10:05.619235Z","iopub.execute_input":"2023-08-10T14:10:05.619611Z","iopub.status.idle":"2023-08-10T14:10:05.626105Z","shell.execute_reply.started":"2023-08-10T14:10:05.619579Z","shell.execute_reply":"2023-08-10T14:10:05.624850Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"(37411, 33)"},"metadata":{}}]},{"cell_type":"code","source":"df1 = df[(df.Label == \"Diversion\") | (df.Label == \"Normal\")].reset_index(drop = True)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:10:05.627963Z","iopub.execute_input":"2023-08-10T14:10:05.629013Z","iopub.status.idle":"2023-08-10T14:10:05.651617Z","shell.execute_reply.started":"2023-08-10T14:10:05.628979Z","shell.execute_reply":"2023-08-10T14:10:05.650510Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"len(df1.Label.unique())","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:10:05.653835Z","iopub.execute_input":"2023-08-10T14:10:05.654365Z","iopub.status.idle":"2023-08-10T14:10:05.662522Z","shell.execute_reply.started":"2023-08-10T14:10:05.654332Z","shell.execute_reply":"2023-08-10T14:10:05.661550Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"code","source":"df1.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:10:05.667213Z","iopub.execute_input":"2023-08-10T14:10:05.668220Z","iopub.status.idle":"2023-08-10T14:10:05.678355Z","shell.execute_reply.started":"2023-08-10T14:10:05.668186Z","shell.execute_reply":"2023-08-10T14:10:05.677127Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"(9388, 33)"},"metadata":{}}]},{"cell_type":"code","source":"df1.Label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:10:05.680109Z","iopub.execute_input":"2023-08-10T14:10:05.680758Z","iopub.status.idle":"2023-08-10T14:10:05.692333Z","shell.execute_reply.started":"2023-08-10T14:10:05.680725Z","shell.execute_reply":"2023-08-10T14:10:05.691182Z"},"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"Diversion    5615\nNormal       3773\nName: Label, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df1 = df1.iloc[1842:]","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:10:05.693820Z","iopub.execute_input":"2023-08-10T14:10:05.694148Z","iopub.status.idle":"2023-08-10T14:10:05.698891Z","shell.execute_reply.started":"2023-08-10T14:10:05.694116Z","shell.execute_reply":"2023-08-10T14:10:05.697737Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"def preprocessing(data): \n    le = LabelEncoder()\n    ohe = OneHotEncoder(drop=\"first\")\n    scaler = MinMaxScaler()\n    num_col = [val for val in data.columns if (data[val].dtype == 'int64')]\n    data[num_col] = scaler.fit_transform(data[num_col])\n    data[\"is_valid\"] = le.fit_transform(data[\"is_valid\"])\n    data[\"Label\"] = le.fit_transform(data[\"Label\"])\n    data = data.join(pd.DataFrame(ohe.fit_transform(data[[\"Switch ID\", \"Port Number\"]]).toarray()))\n    data = data.drop(columns = [\"Switch ID\", \"Port Number\"])\n    return data","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:10:05.700724Z","iopub.execute_input":"2023-08-10T14:10:05.701173Z","iopub.status.idle":"2023-08-10T14:10:05.709547Z","shell.execute_reply.started":"2023-08-10T14:10:05.701142Z","shell.execute_reply":"2023-08-10T14:10:05.708494Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"new_df = preprocessing(df1)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:10:05.711142Z","iopub.execute_input":"2023-08-10T14:10:05.711618Z","iopub.status.idle":"2023-08-10T14:10:05.750909Z","shell.execute_reply.started":"2023-08-10T14:10:05.711587Z","shell.execute_reply":"2023-08-10T14:10:05.749946Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"new_df.Label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:10:05.754279Z","iopub.execute_input":"2023-08-10T14:10:05.754632Z","iopub.status.idle":"2023-08-10T14:10:05.762775Z","shell.execute_reply.started":"2023-08-10T14:10:05.754593Z","shell.execute_reply":"2023-08-10T14:10:05.761789Z"},"trusted":true},"execution_count":66,"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"0    3773\n1    3773\nName: Label, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"new_df.Label.unique()","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:10:05.764560Z","iopub.execute_input":"2023-08-10T14:10:05.765344Z","iopub.status.idle":"2023-08-10T14:10:05.773500Z","shell.execute_reply.started":"2023-08-10T14:10:05.765311Z","shell.execute_reply":"2023-08-10T14:10:05.772521Z"},"trusted":true},"execution_count":67,"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"array([0, 1])"},"metadata":{}}]},{"cell_type":"code","source":"n = new_df.shape[0]\nnp.random.seed(42)\nidx = np.arange(n)\nnp.random.shuffle(idx)\nnew_df = new_df.iloc[idx]\nnew_df.reset_index(drop = True, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:10:05.775169Z","iopub.execute_input":"2023-08-10T14:10:05.775578Z","iopub.status.idle":"2023-08-10T14:10:05.785543Z","shell.execute_reply.started":"2023-08-10T14:10:05.775546Z","shell.execute_reply":"2023-08-10T14:10:05.784624Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"ohe = OneHotEncoder()\nnew_labels = ohe.fit_transform(new_df[[\"Label\"]])","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:10:05.787352Z","iopub.execute_input":"2023-08-10T14:10:05.787936Z","iopub.status.idle":"2023-08-10T14:10:05.796077Z","shell.execute_reply.started":"2023-08-10T14:10:05.787904Z","shell.execute_reply":"2023-08-10T14:10:05.794886Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"new_data = new_df.drop(columns = \"Label\")","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:10:05.798524Z","iopub.execute_input":"2023-08-10T14:10:05.799572Z","iopub.status.idle":"2023-08-10T14:10:05.805646Z","shell.execute_reply.started":"2023-08-10T14:10:05.799541Z","shell.execute_reply":"2023-08-10T14:10:05.804775Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"new_data.shape, new_df.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:10:05.806891Z","iopub.execute_input":"2023-08-10T14:10:05.807315Z","iopub.status.idle":"2023-08-10T14:10:05.817790Z","shell.execute_reply.started":"2023-08-10T14:10:05.807284Z","shell.execute_reply":"2023-08-10T14:10:05.816471Z"},"trusted":true},"execution_count":71,"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"((7546, 40), (7546, 41))"},"metadata":{}}]},{"cell_type":"code","source":"input_dim = new_df.shape[1]-1\noutput_dim = input_dim\nnum_classes = len(new_df.Label.unique())\n#batch_size = 128\ninput_dim, output_dim, num_classes","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:10:05.821323Z","iopub.execute_input":"2023-08-10T14:10:05.821629Z","iopub.status.idle":"2023-08-10T14:10:05.829736Z","shell.execute_reply.started":"2023-08-10T14:10:05.821594Z","shell.execute_reply":"2023-08-10T14:10:05.828743Z"},"trusted":true},"execution_count":72,"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"(40, 40, 2)"},"metadata":{}}]},{"cell_type":"code","source":"# Define the generator\nclass IDSGenerator(nn.Module):\n    def __init__(self, input_dim, num_classes, output_dim):\n        super(IDSGenerator, self).__init__()\n        self.fc1 = nn.Linear(102, 256)\n        #self.fc2 = nn.Linear(256, 512)\n        #self.fc3 = nn.Linear(512, 1024)\n        self.fc2 = nn.Linear(256, output_dim)\n\n    def forward(self, x, labels):\n        if len(labels.shape) == 1:\n            labels = labels.unsqueeze(-1)\n        elif len(labels.shape) == 3:\n            labels = labels.squeeze(dim = 1)\n        x = torch.cat([x, labels], 1)\n        x = F.relu(self.fc1(x))\n        #x = F.relu(self.fc2(x))\n        #x = F.relu(self.fc3(x))\n        x = torch.tanh(self.fc2(x))\n        return x\n\n# Define the discriminator\nclass IDSDiscriminator(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(IDSDiscriminator, self).__init__()\n        self.fc1 = nn.Linear(input_dim + num_classes, 256)\n        #self.fc2 = nn.Linear(1024, 512)\n        #self.fc3 = nn.Linear(512, 256)\n        self.fc2 = nn.Linear(256, 1)\n\n    def forward(self, x, labels):\n        if len(labels.shape) == 1:\n            labels = labels.unsqueeze(-1)\n        elif len(labels.shape) == 3:\n            labels = labels.squeeze(dim = 1)\n        labels = labels.float()\n        x = torch.cat([x, labels], 1)\n        x = F.leaky_relu(self.fc1(x))\n        #x = F.leaky_relu(self.fc2(x))\n        #x = F.leaky_relu(self.fc3(x))\n        x = torch.sigmoid(self.fc2(x))\n        return x\n\n# Define the IDS model\nclass IDSModel(nn.Module):\n    def __init__(self, input_dim, num_classes, output_dim):\n        super(IDSModel, self).__init__()\n        self.generator = IDSGenerator(input_dim, num_classes, output_dim)\n        self.discriminator = IDSDiscriminator(input_dim, num_classes)\n\n    def forward(self, x, labels=None): \n        #gen_output = self.generator(x)\n        #disc_real_output = self.discriminator(x)\n        #disc_fake_output = self.discriminator(gen_output)\n        #return disc_real_output, disc_fake_output, gen_output\n        if self.training: \n            # In training mode, we pass the labels to both generator and discriminator\n            z = torch.randn(x.size(0), input_dim)\n            gen_labels = torch.randint(0, num_classes, (x.size(0),))\n            gen_data = self.generator(z, gen_labels)\n            disc_scores = self.discriminator(x, labels)\n            gen_scores = self.discriminator(gen_data, gen_labels)\n            return disc_scores, gen_scores\n        else:\n            # In evaluation mode, we only generate data, so we only need labels for the generator\n            gen_data = self.generator(x, labels)\n            return gen_data\n","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:10:05.834997Z","iopub.execute_input":"2023-08-10T14:10:05.835749Z","iopub.status.idle":"2023-08-10T14:10:05.851565Z","shell.execute_reply.started":"2023-08-10T14:10:05.835718Z","shell.execute_reply":"2023-08-10T14:10:05.850269Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, dataframe, labels):\n        self.dataframe = dataframe\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, index):\n        #features = torch.tensor(new_df.drop(columns = \"Label\").iloc[index].values, device = self.device).float()\n        self.dataframe = self.dataframe.apply(pd.to_numeric, errors='coerce')\n        features = torch.from_numpy(self.dataframe.iloc[index].values).float()\n        #label = torch.tensor(self.dataframe.iloc[index][\"Label\"], device = self.device).float()\n        labels = torch.from_numpy(self.labels[index].toarray())\n        return features, labels","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:10:05.854770Z","iopub.execute_input":"2023-08-10T14:10:05.855329Z","iopub.status.idle":"2023-08-10T14:10:05.864268Z","shell.execute_reply.started":"2023-08-10T14:10:05.855303Z","shell.execute_reply":"2023-08-10T14:10:05.863590Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"new_data[[val for val in new_data.columns if type(val) != str]] = new_data[[val for val in new_data.columns if type(val) != str]].fillna(new_data[[val for val in new_data.columns if type(val) != str]].median())","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:10:05.865381Z","iopub.execute_input":"2023-08-10T14:10:05.866111Z","iopub.status.idle":"2023-08-10T14:10:05.888812Z","shell.execute_reply.started":"2023-08-10T14:10:05.866078Z","shell.execute_reply":"2023-08-10T14:10:05.887783Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"new_data","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:10:05.892114Z","iopub.execute_input":"2023-08-10T14:10:05.892370Z","iopub.status.idle":"2023-08-10T14:10:05.938817Z","shell.execute_reply.started":"2023-08-10T14:10:05.892347Z","shell.execute_reply":"2023-08-10T14:10:05.937692Z"},"trusted":true},"execution_count":76,"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"      Received Packets  Received Bytes  Sent Bytes  Sent Packets  \\\n0             0.009227        0.209556    0.132499      0.006193   \n1             0.012536        0.233205    0.211947      0.008938   \n2             0.006512        0.162736    0.332483      0.983834   \n3             0.997460        0.147799    0.380557      0.509762   \n4             0.010809        0.325252    0.343817      0.012405   \n...                ...             ...         ...           ...   \n7541          0.889012        0.071702    0.107921      0.745353   \n7542          0.866710        0.094214    0.630518      0.683489   \n7543          0.016793        0.580628    0.441993      0.981784   \n7544          0.000388        0.000034    0.026353      0.000465   \n7545          0.340562        0.158080    0.292974      0.753328   \n\n      Port alive Duration (S)  Packets Rx Dropped  Packets Tx Dropped  \\\n0                    0.440965                 0.0                 0.0   \n1                    0.796117                 0.0                 0.0   \n2                    0.383025                 0.0                 0.0   \n3                    0.736925                 0.0                 0.0   \n4                    0.752584                 0.0                 0.0   \n...                       ...                 ...                 ...   \n7541                 0.226433                 0.0                 0.0   \n7542                 0.736925                 0.0                 0.0   \n7543                 0.768243                 0.0                 0.0   \n7544                 0.000000                 0.0                 0.0   \n7545                 0.430003                 0.0                 0.0   \n\n      Packets Rx Errors  Packets Tx Errors  Delta Received Packets  ...    0  \\\n0                   0.0                0.0                0.263473  ...  0.0   \n1                   0.0                0.0                0.023952  ...  0.0   \n2                   0.0                0.0                0.604790  ...  0.0   \n3                   0.0                0.0                0.023952  ...  0.0   \n4                   0.0                0.0                0.724551  ...  0.0   \n...                 ...                ...                     ...  ...  ...   \n7541                0.0                0.0                0.000000  ...  0.0   \n7542                0.0                0.0                0.023952  ...  0.0   \n7543                0.0                0.0                0.000000  ...  0.0   \n7544                0.0                0.0                0.425150  ...  0.0   \n7545                0.0                0.0                0.023952  ...  0.0   \n\n        1    2    3    4    5    6    7    8    9  \n0     0.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  \n1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n3     0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  \n4     1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n...   ...  ...  ...  ...  ...  ...  ...  ...  ...  \n7541  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n7542  0.0  0.0  0.0  0.0  0.0  1.0  0.0  1.0  0.0  \n7543  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  \n7544  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n7545  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n\n[7546 rows x 40 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Received Packets</th>\n      <th>Received Bytes</th>\n      <th>Sent Bytes</th>\n      <th>Sent Packets</th>\n      <th>Port alive Duration (S)</th>\n      <th>Packets Rx Dropped</th>\n      <th>Packets Tx Dropped</th>\n      <th>Packets Rx Errors</th>\n      <th>Packets Tx Errors</th>\n      <th>Delta Received Packets</th>\n      <th>...</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.009227</td>\n      <td>0.209556</td>\n      <td>0.132499</td>\n      <td>0.006193</td>\n      <td>0.440965</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.263473</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.012536</td>\n      <td>0.233205</td>\n      <td>0.211947</td>\n      <td>0.008938</td>\n      <td>0.796117</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.023952</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.006512</td>\n      <td>0.162736</td>\n      <td>0.332483</td>\n      <td>0.983834</td>\n      <td>0.383025</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.604790</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.997460</td>\n      <td>0.147799</td>\n      <td>0.380557</td>\n      <td>0.509762</td>\n      <td>0.736925</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.023952</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.010809</td>\n      <td>0.325252</td>\n      <td>0.343817</td>\n      <td>0.012405</td>\n      <td>0.752584</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.724551</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7541</th>\n      <td>0.889012</td>\n      <td>0.071702</td>\n      <td>0.107921</td>\n      <td>0.745353</td>\n      <td>0.226433</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7542</th>\n      <td>0.866710</td>\n      <td>0.094214</td>\n      <td>0.630518</td>\n      <td>0.683489</td>\n      <td>0.736925</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.023952</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7543</th>\n      <td>0.016793</td>\n      <td>0.580628</td>\n      <td>0.441993</td>\n      <td>0.981784</td>\n      <td>0.768243</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7544</th>\n      <td>0.000388</td>\n      <td>0.000034</td>\n      <td>0.026353</td>\n      <td>0.000465</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.425150</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7545</th>\n      <td>0.340562</td>\n      <td>0.158080</td>\n      <td>0.292974</td>\n      <td>0.753328</td>\n      <td>0.430003</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.023952</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>7546 rows Ã— 40 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"new_data.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:10:05.940116Z","iopub.execute_input":"2023-08-10T14:10:05.940831Z","iopub.status.idle":"2023-08-10T14:10:05.951845Z","shell.execute_reply.started":"2023-08-10T14:10:05.940798Z","shell.execute_reply":"2023-08-10T14:10:05.950939Z"},"trusted":true},"execution_count":77,"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"Received Packets                 0\nReceived Bytes                   0\nSent Bytes                       0\nSent Packets                     0\nPort alive Duration (S)          0\nPackets Rx Dropped               0\nPackets Tx Dropped               0\nPackets Rx Errors                0\nPackets Tx Errors                0\nDelta Received Packets           0\nDelta Received Bytes             0\nDelta Sent Bytes                 0\nDelta Sent Packets               0\nDelta Port alive Duration (S)    0\nDelta Packets Rx Dropped         0\n Delta Packets Tx Dropped        0\nDelta Packets Rx Errors          0\nDelta Packets Tx Errors          0\nConnection Point                 0\nTotal Load/Rate                  0\nTotal Load/Latest                0\nUnknown Load/Rate                0\nUnknown Load/Latest              0\nLatest bytes counter             0\nis_valid                         0\nTable ID                         0\nActive Flow Entries              0\nPackets Looked Up                0\nPackets Matched                  0\nMax Size                         0\n0                                0\n1                                0\n2                                0\n3                                0\n4                                0\n5                                0\n6                                0\n7                                0\n8                                0\n9                                0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"dataset = MyDataset(new_data, new_labels)\ndataloader = DataLoader(dataset, batch_size=32, shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:10:05.953127Z","iopub.execute_input":"2023-08-10T14:10:05.953881Z","iopub.status.idle":"2023-08-10T14:10:05.959692Z","shell.execute_reply.started":"2023-08-10T14:10:05.953849Z","shell.execute_reply":"2023-08-10T14:10:05.958582Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:10:05.961324Z","iopub.execute_input":"2023-08-10T14:10:05.961725Z","iopub.status.idle":"2023-08-10T14:10:05.968101Z","shell.execute_reply.started":"2023-08-10T14:10:05.961689Z","shell.execute_reply":"2023-08-10T14:10:05.967056Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"# Initialize the models\n#generator = IDSGenerator()\n#discriminator = IDSDiscriminator()\n#model = IDSModel(generator, discriminator)\nmodel = IDSModel(input_dim, num_classes, output_dim)\nmodel = model.to(device).float()\n\n# Define loss function and optimizers\ncriterion = nn.BCELoss()\noptimizer_G = torch.optim.Adam(model.generator.parameters(), lr=0.0001)\noptimizer_D = torch.optim.Adam(model.discriminator.parameters(), lr=0.0001)\n#optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n\n# Set the number of epochs\nn_epochs = 100\nz_dim = 100\n\n# Training loop\nfor epoch in range(n_epochs):\n    for i, (data, labels) in enumerate(dataloader):\n        data, labels = data.to(device), labels.to(device)  # move data and labels to device\n        batch_size = len(data)\n        #real_labels = torch.ones((batch_size, 1)).to(device)\n        #fake_labels = torch.zeros((batch_size, 1)).to(device)\n        \n        # Train the discriminator\n        optimizer_D.zero_grad()\n        z = torch.randn(batch_size, z_dim).to(device)\n        gen_labels = torch.randint(0, num_classes, (batch_size,)).to(device).float()  \n        gen_labels_one_hot = F.one_hot(gen_labels.to(torch.int64), num_classes=2)    \n        gen_data = model.generator(z, gen_labels_one_hot)\n        real_scores = model.discriminator(data, labels)\n        fake_scores = model.discriminator(gen_data.detach(), gen_labels_one_hot)\n        d_loss = -torch.mean(real_scores) + torch.mean(fake_scores)\n        d_loss.backward()\n        optimizer_D.step()\n        for p in model.discriminator.parameters(): p.data.clamp_(-0.01, 0.01)\n            \n        # Train the generator \n        optimizer_G.zero_grad()\n        z = torch.randn(batch_size, z_dim).to(device)\n        gen_labels = torch.randint(0, num_classes, (batch_size,)).to(device).float()\n        gen_labels_one_hot = F.one_hot(gen_labels.to(torch.int64), num_classes = 2)\n        gen_data = model.generator(z, gen_labels_one_hot)\n        fake_scores = model.discriminator(gen_data, gen_labels_one_hot)\n        g_loss = -torch.mean(fake_scores)\n        g_loss.backward()\n        optimizer_G.step()\n        \n        #real_scores = model.discriminator(data, labels)\n        #d_loss_real = criterion(real_scores, real_labels)\n        #fake_scores = model.discriminator(gen_data.detach(), gen_labels_one_hot)\n        #d_loss_fake = criterion(fake_scores, fake_labels)\n        #d_loss = d_loss_real + d_loss_fake\n        #d_loss.backward()\n        #optimizer_D.step()\n        \n        # Train the generator\n        #optimizer_G.zero_grad()\n        #z = torch.randn(batch_size, z_dim).to(device)\n        #gen_labels = torch.randint(0, num_classes, (batch_size,)).to(device).float()  # generate random labels for generator\n        #gen_labels_one_hot = F.one_hot(gen_labels.to(torch.int64), num_classes=2)    # Get a one-hot tensor of size ([128, 2])\n        #gen_data = model.generator(z, gen_labels_one_hot)\n        #gen_scores = model.discriminator(gen_data, gen_labels_one_hot)\n        #g_loss = criterion(gen_scores, real_labels)\n        #g_loss.backward()\n        #optimizer_G.step()\n\n    # Print losses for every epoch\n    print(f'Epoch {epoch+1}/{n_epochs}:\\tDiscriminator Loss: {d_loss}\\t\\tGenerator Loss: {g_loss}')","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:10:05.970011Z","iopub.execute_input":"2023-08-10T14:10:05.970402Z","iopub.status.idle":"2023-08-10T15:26:14.100888Z","shell.execute_reply.started":"2023-08-10T14:10:05.970371Z","shell.execute_reply":"2023-08-10T15:26:14.099369Z"},"trusted":true},"execution_count":80,"outputs":[{"name":"stdout","text":"Epoch 1/100:\tDiscriminator Loss: -0.006105989217758179\t\tGenerator Loss: -0.49659305810928345\nEpoch 2/100:\tDiscriminator Loss: -0.0012176036834716797\t\tGenerator Loss: -0.5018243789672852\nEpoch 3/100:\tDiscriminator Loss: -0.0015788674354553223\t\tGenerator Loss: -0.5011132955551147\nEpoch 4/100:\tDiscriminator Loss: -0.0017507076263427734\t\tGenerator Loss: -0.5020782351493835\nEpoch 5/100:\tDiscriminator Loss: -0.0038729608058929443\t\tGenerator Loss: -0.4991409182548523\nEpoch 6/100:\tDiscriminator Loss: -0.001195669174194336\t\tGenerator Loss: -0.49960872530937195\nEpoch 7/100:\tDiscriminator Loss: 0.0005388855934143066\t\tGenerator Loss: -0.5022485256195068\nEpoch 8/100:\tDiscriminator Loss: 0.001846611499786377\t\tGenerator Loss: -0.5022285580635071\nEpoch 9/100:\tDiscriminator Loss: -0.0016911625862121582\t\tGenerator Loss: -0.4974249601364136\nEpoch 10/100:\tDiscriminator Loss: -0.001763850450515747\t\tGenerator Loss: -0.49796774983406067\nEpoch 11/100:\tDiscriminator Loss: -0.0005200207233428955\t\tGenerator Loss: -0.49936169385910034\nEpoch 12/100:\tDiscriminator Loss: -0.0005072951316833496\t\tGenerator Loss: -0.5002066493034363\nEpoch 13/100:\tDiscriminator Loss: 0.00013065338134765625\t\tGenerator Loss: -0.5014163255691528\nEpoch 14/100:\tDiscriminator Loss: -0.0004966855049133301\t\tGenerator Loss: -0.4998599886894226\nEpoch 15/100:\tDiscriminator Loss: 0.0005981922149658203\t\tGenerator Loss: -0.5013664960861206\nEpoch 16/100:\tDiscriminator Loss: -0.0009149014949798584\t\tGenerator Loss: -0.498949259519577\nEpoch 17/100:\tDiscriminator Loss: -0.0018079280853271484\t\tGenerator Loss: -0.4992187023162842\nEpoch 18/100:\tDiscriminator Loss: 0.00019931793212890625\t\tGenerator Loss: -0.5004332065582275\nEpoch 19/100:\tDiscriminator Loss: 0.0011656880378723145\t\tGenerator Loss: -0.5016487836837769\nEpoch 20/100:\tDiscriminator Loss: -0.000748753547668457\t\tGenerator Loss: -0.5011045932769775\nEpoch 21/100:\tDiscriminator Loss: -9.912252426147461e-05\t\tGenerator Loss: -0.5001484155654907\nEpoch 22/100:\tDiscriminator Loss: -0.00025457143783569336\t\tGenerator Loss: -0.5003072619438171\nEpoch 23/100:\tDiscriminator Loss: -0.00042051076889038086\t\tGenerator Loss: -0.5012103319168091\nEpoch 24/100:\tDiscriminator Loss: 0.0011014342308044434\t\tGenerator Loss: -0.5019242167472839\nEpoch 25/100:\tDiscriminator Loss: -0.0010139644145965576\t\tGenerator Loss: -0.499634325504303\nEpoch 26/100:\tDiscriminator Loss: -0.0015532076358795166\t\tGenerator Loss: -0.49879875779151917\nEpoch 27/100:\tDiscriminator Loss: -0.0012173354625701904\t\tGenerator Loss: -0.4990527927875519\nEpoch 28/100:\tDiscriminator Loss: 0.00039386749267578125\t\tGenerator Loss: -0.4995644688606262\nEpoch 29/100:\tDiscriminator Loss: 0.0014307200908660889\t\tGenerator Loss: -0.49925944209098816\nEpoch 30/100:\tDiscriminator Loss: -0.0018737316131591797\t\tGenerator Loss: -0.4949178993701935\nEpoch 31/100:\tDiscriminator Loss: -0.0008338093757629395\t\tGenerator Loss: -0.49714207649230957\nEpoch 32/100:\tDiscriminator Loss: -0.0007641017436981201\t\tGenerator Loss: -0.49607163667678833\nEpoch 33/100:\tDiscriminator Loss: -7.930397987365723e-05\t\tGenerator Loss: -0.49839985370635986\nEpoch 34/100:\tDiscriminator Loss: 0.002998173236846924\t\tGenerator Loss: -0.5082532167434692\nEpoch 35/100:\tDiscriminator Loss: -0.0022734403610229492\t\tGenerator Loss: -0.5064612030982971\nEpoch 36/100:\tDiscriminator Loss: -0.0021867752075195312\t\tGenerator Loss: -0.49642375111579895\nEpoch 37/100:\tDiscriminator Loss: 0.0006304383277893066\t\tGenerator Loss: -0.5036013722419739\nEpoch 38/100:\tDiscriminator Loss: 1.233816146850586e-05\t\tGenerator Loss: -0.5003258585929871\nEpoch 39/100:\tDiscriminator Loss: -0.0003045797348022461\t\tGenerator Loss: -0.5019598603248596\nEpoch 40/100:\tDiscriminator Loss: -0.00017118453979492188\t\tGenerator Loss: -0.500819742679596\nEpoch 41/100:\tDiscriminator Loss: -0.0002396106719970703\t\tGenerator Loss: -0.5007238984107971\nEpoch 42/100:\tDiscriminator Loss: -6.058812141418457e-05\t\tGenerator Loss: -0.4999600350856781\nEpoch 43/100:\tDiscriminator Loss: -0.0007189512252807617\t\tGenerator Loss: -0.5002458691596985\nEpoch 44/100:\tDiscriminator Loss: -0.00034356117248535156\t\tGenerator Loss: -0.5001566410064697\nEpoch 45/100:\tDiscriminator Loss: -0.0014021992683410645\t\tGenerator Loss: -0.49824291467666626\nEpoch 46/100:\tDiscriminator Loss: -0.001269996166229248\t\tGenerator Loss: -0.4987952411174774\nEpoch 47/100:\tDiscriminator Loss: -0.0006651878356933594\t\tGenerator Loss: -0.500198483467102\nEpoch 48/100:\tDiscriminator Loss: -0.00013333559036254883\t\tGenerator Loss: -0.5012993812561035\nEpoch 49/100:\tDiscriminator Loss: 0.0012910962104797363\t\tGenerator Loss: -0.5018410682678223\nEpoch 50/100:\tDiscriminator Loss: -0.0010498464107513428\t\tGenerator Loss: -0.49836263060569763\nEpoch 51/100:\tDiscriminator Loss: 0.0005788207054138184\t\tGenerator Loss: -0.49995601177215576\nEpoch 52/100:\tDiscriminator Loss: -0.00022464990615844727\t\tGenerator Loss: -0.49940645694732666\nEpoch 53/100:\tDiscriminator Loss: -0.0008726119995117188\t\tGenerator Loss: -0.49839478731155396\nEpoch 54/100:\tDiscriminator Loss: 0.00030052661895751953\t\tGenerator Loss: -0.5005887150764465\nEpoch 55/100:\tDiscriminator Loss: -0.00012254714965820312\t\tGenerator Loss: -0.49921661615371704\nEpoch 56/100:\tDiscriminator Loss: -0.0004899799823760986\t\tGenerator Loss: -0.4992232918739319\nEpoch 57/100:\tDiscriminator Loss: 0.00040602684020996094\t\tGenerator Loss: -0.5003973245620728\nEpoch 58/100:\tDiscriminator Loss: -0.0005030035972595215\t\tGenerator Loss: -0.5001596808433533\nEpoch 59/100:\tDiscriminator Loss: -0.00032722949981689453\t\tGenerator Loss: -0.5010003447532654\nEpoch 60/100:\tDiscriminator Loss: 0.0005057752132415771\t\tGenerator Loss: -0.4986880421638489\nEpoch 61/100:\tDiscriminator Loss: -0.0003674626350402832\t\tGenerator Loss: -0.49918946623802185\nEpoch 62/100:\tDiscriminator Loss: -0.00014823675155639648\t\tGenerator Loss: -0.5020357966423035\nEpoch 63/100:\tDiscriminator Loss: 3.0219554901123047e-05\t\tGenerator Loss: -0.5001311302185059\nEpoch 64/100:\tDiscriminator Loss: -0.00010851025581359863\t\tGenerator Loss: -0.49888938665390015\nEpoch 65/100:\tDiscriminator Loss: 0.0003230571746826172\t\tGenerator Loss: -0.4990554451942444\nEpoch 66/100:\tDiscriminator Loss: 0.0004590153694152832\t\tGenerator Loss: -0.5009793043136597\nEpoch 67/100:\tDiscriminator Loss: 0.0019121766090393066\t\tGenerator Loss: -0.5023888945579529\nEpoch 68/100:\tDiscriminator Loss: 0.0013615190982818604\t\tGenerator Loss: -0.5015585422515869\nEpoch 69/100:\tDiscriminator Loss: -0.001050710678100586\t\tGenerator Loss: -0.5009459257125854\nEpoch 70/100:\tDiscriminator Loss: 0.00020551681518554688\t\tGenerator Loss: -0.5000446438789368\nEpoch 71/100:\tDiscriminator Loss: -0.00031828880310058594\t\tGenerator Loss: -0.4985191524028778\nEpoch 72/100:\tDiscriminator Loss: -0.00033915042877197266\t\tGenerator Loss: -0.5002720355987549\nEpoch 73/100:\tDiscriminator Loss: -0.00048285722732543945\t\tGenerator Loss: -0.4991973042488098\nEpoch 74/100:\tDiscriminator Loss: -0.0006213486194610596\t\tGenerator Loss: -0.4992545247077942\nEpoch 75/100:\tDiscriminator Loss: -0.0008016526699066162\t\tGenerator Loss: -0.49942252039909363\nEpoch 76/100:\tDiscriminator Loss: -0.001370251178741455\t\tGenerator Loss: -0.4991821348667145\nEpoch 77/100:\tDiscriminator Loss: -0.0013899803161621094\t\tGenerator Loss: -0.4999050199985504\nEpoch 78/100:\tDiscriminator Loss: -0.0009006261825561523\t\tGenerator Loss: -0.5024835467338562\nEpoch 79/100:\tDiscriminator Loss: -0.0012165307998657227\t\tGenerator Loss: -0.5022894740104675\nEpoch 80/100:\tDiscriminator Loss: -0.002237677574157715\t\tGenerator Loss: -0.5021418929100037\nEpoch 81/100:\tDiscriminator Loss: -0.0012081265449523926\t\tGenerator Loss: -0.5024434924125671\nEpoch 82/100:\tDiscriminator Loss: -0.0008764564990997314\t\tGenerator Loss: -0.49848219752311707\nEpoch 83/100:\tDiscriminator Loss: 0.0001239180564880371\t\tGenerator Loss: -0.5039269924163818\nEpoch 84/100:\tDiscriminator Loss: -0.0016964077949523926\t\tGenerator Loss: -0.49891501665115356\nEpoch 85/100:\tDiscriminator Loss: -0.000462949275970459\t\tGenerator Loss: -0.49951815605163574\nEpoch 86/100:\tDiscriminator Loss: -0.00025969743728637695\t\tGenerator Loss: -0.5009905099868774\nEpoch 87/100:\tDiscriminator Loss: -0.0005035102367401123\t\tGenerator Loss: -0.49880197644233704\nEpoch 88/100:\tDiscriminator Loss: -0.0009370148181915283\t\tGenerator Loss: -0.49957966804504395\nEpoch 89/100:\tDiscriminator Loss: -0.0005857348442077637\t\tGenerator Loss: -0.49905911087989807\nEpoch 90/100:\tDiscriminator Loss: -0.00039756298065185547\t\tGenerator Loss: -0.4988000690937042\nEpoch 91/100:\tDiscriminator Loss: -0.0007710158824920654\t\tGenerator Loss: -0.49863794445991516\nEpoch 92/100:\tDiscriminator Loss: 0.0001678466796875\t\tGenerator Loss: -0.4986308217048645\nEpoch 93/100:\tDiscriminator Loss: -0.0007357001304626465\t\tGenerator Loss: -0.5000181198120117\nEpoch 94/100:\tDiscriminator Loss: -0.0002694129943847656\t\tGenerator Loss: -0.5034787654876709\nEpoch 95/100:\tDiscriminator Loss: 0.00029087066650390625\t\tGenerator Loss: -0.4983070194721222\nEpoch 96/100:\tDiscriminator Loss: 4.0411949157714844e-05\t\tGenerator Loss: -0.5001553893089294\nEpoch 97/100:\tDiscriminator Loss: 0.0013491511344909668\t\tGenerator Loss: -0.503510057926178\nEpoch 98/100:\tDiscriminator Loss: -0.0013030171394348145\t\tGenerator Loss: -0.5002992749214172\nEpoch 99/100:\tDiscriminator Loss: -0.0011799335479736328\t\tGenerator Loss: -0.4970393478870392\nEpoch 100/100:\tDiscriminator Loss: 9.40561294555664e-05\t\tGenerator Loss: -0.49883273243904114\n","output_type":"stream"}]},{"cell_type":"code","source":"num_samples = 1000  # specify the number of samples you want\nnoise_dim = 100  # the dimension of the input noise vector, adjust according to your model\nnoise = torch.randn(num_samples, noise_dim).to(device)\nlabels = torch.randint(0, num_classes, (num_samples, )).float().to(device)\n#labels = labels.float().unsqueeze(1).to(device)\nlabels_onehot = F.one_hot(labels.to(torch.int64), num_classes=2).float()\n#inputs = torch.cat((noise,labels), -1)\n\nwith torch.no_grad():  # we don't need gradients for this part\n    synthetic_data = model.generator(noise, labels_onehot)\n    \nsynthetic_data = synthetic_data.cpu().numpy()  # move data to cpu and convert to numpy array\nsynthetic_df = pd.DataFrame(synthetic_data)  # convert to DataFrame","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:26:14.102545Z","iopub.execute_input":"2023-08-10T15:26:14.102892Z","iopub.status.idle":"2023-08-10T15:26:14.113744Z","shell.execute_reply.started":"2023-08-10T15:26:14.102859Z","shell.execute_reply":"2023-08-10T15:26:14.112747Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"F.one_hot(labels.to(torch.int64), num_classes = 2).float()","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:26:14.115668Z","iopub.execute_input":"2023-08-10T15:26:14.115933Z","iopub.status.idle":"2023-08-10T15:26:14.134837Z","shell.execute_reply.started":"2023-08-10T15:26:14.115909Z","shell.execute_reply":"2023-08-10T15:26:14.133832Z"},"trusted":true},"execution_count":82,"outputs":[{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"tensor([[0., 1.],\n        [1., 0.],\n        [0., 1.],\n        ...,\n        [0., 1.],\n        [0., 1.],\n        [1., 0.]], device='cuda:0')"},"metadata":{}}]},{"cell_type":"code","source":"test1 = pd.DataFrame(torch.cat((model.generator(noise, labels_onehot), labels_onehot), -1).cpu().detach().numpy())","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:26:14.140500Z","iopub.execute_input":"2023-08-10T15:26:14.140772Z","iopub.status.idle":"2023-08-10T15:26:14.146725Z","shell.execute_reply.started":"2023-08-10T15:26:14.140749Z","shell.execute_reply":"2023-08-10T15:26:14.145602Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"labels","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:26:14.148116Z","iopub.execute_input":"2023-08-10T15:26:14.148709Z","iopub.status.idle":"2023-08-10T15:26:14.190009Z","shell.execute_reply.started":"2023-08-10T15:26:14.148675Z","shell.execute_reply":"2023-08-10T15:26:14.189059Z"},"trusted":true},"execution_count":84,"outputs":[{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"tensor([1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1.,\n        1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n        1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n        1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n        0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0.,\n        1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n        1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n        0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1.,\n        0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0.,\n        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0.,\n        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n        0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n        1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1.,\n        1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n        0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n        0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n        0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0.,\n        1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0.,\n        1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0.,\n        0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1.,\n        1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0.,\n        1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n        1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0.,\n        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n        0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,\n        1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0.,\n        0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n        0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0.,\n        0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0.,\n        1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n        1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n        0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n        1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0.,\n        0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n        0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0.,\n        0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,\n        0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n        1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0.,\n        0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0.,\n        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.,\n        1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n        1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n        1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0.,\n        0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0.,\n        1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n        1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0.,\n        0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1.,\n        0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1.,\n        0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n        0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1.,\n        0., 1., 0., 0., 0., 1., 0., 1., 1., 0.], device='cuda:0')"},"metadata":{}}]},{"cell_type":"code","source":"test1","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:26:14.191267Z","iopub.execute_input":"2023-08-10T15:26:14.191592Z","iopub.status.idle":"2023-08-10T15:26:14.222074Z","shell.execute_reply.started":"2023-08-10T15:26:14.191562Z","shell.execute_reply":"2023-08-10T15:26:14.221098Z"},"trusted":true},"execution_count":85,"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"           0         1         2         3         4         5         6   \\\n0    0.537974  0.135781  0.156351  0.574102  0.533220 -0.158636 -0.101433   \n1   -0.004974  0.196055  0.178136  0.169558  0.453874 -0.018763 -0.040355   \n2    0.498937  0.402633  0.405978  0.595868  0.717451 -0.152932 -0.158300   \n3    0.176377  0.066691  0.175304  0.460799  0.465094 -0.012672  0.098899   \n4   -0.088944  0.163707  0.152545 -0.121473  0.397098 -0.021557  0.087762   \n..        ...       ...       ...       ...       ...       ...       ...   \n995  0.648247  0.441736  0.216079  0.641411  0.700321  0.092613  0.160054   \n996  0.087570  0.141916  0.069388 -0.145960  0.415810 -0.028320 -0.122977   \n997  0.689149  0.576419  0.431294  0.793375  0.794358  0.135369  0.097210   \n998  0.242202  0.242578  0.245547  0.525288  0.384736  0.007170 -0.034059   \n999 -0.367847  0.346774  0.333824 -0.174182  0.782559  0.017074  0.020070   \n\n           7         8         9   ...        32        33        34  \\\n0   -0.032645 -0.105989 -0.063549  ...  0.047417 -0.149233 -0.432112   \n1    0.012371 -0.003687 -0.051264  ...  0.267803 -0.022550  0.261758   \n2   -0.043406 -0.252996 -0.062804  ...  0.362044  0.077862  0.012036   \n3    0.090992  0.015092 -0.038908  ...  0.044669 -0.056171  0.157613   \n4    0.077328  0.053831  0.060876  ...  0.332996 -0.046965  0.193844   \n..        ...       ...       ...  ...       ...       ...       ...   \n995  0.050411  0.083747  0.334964  ...  0.359591 -0.136777  0.515093   \n996  0.023025 -0.050155  0.104348  ...  0.269484 -0.066122  0.412675   \n997  0.124003 -0.053644 -0.115828  ...  0.535664 -0.074024  0.086301   \n998  0.000674  0.036789  0.092761  ...  0.015867 -0.076312  0.091667   \n999  0.003414  0.056128  0.030258  ...  0.297961  0.075676  0.454723   \n\n           35        36        37        38        39   40   41  \n0    0.144402  0.430720  0.462198  0.039850  0.225659  0.0  1.0  \n1    0.067600  0.342194  0.498299 -0.174735  0.262961  1.0  0.0  \n2    0.035554  0.371593  0.464975 -0.085389  0.151620  0.0  1.0  \n3    0.074912 -0.193162 -0.035965  0.159661  0.245726  0.0  1.0  \n4   -0.001762  0.387252  0.351752  0.175914 -0.013901  1.0  0.0  \n..        ...       ...       ...       ...       ...  ...  ...  \n995  0.154892  0.330730  0.593833  0.009254 -0.097751  0.0  1.0  \n996  0.342117 -0.363268  0.547810  0.072129  0.068804  1.0  0.0  \n997  0.294766 -0.107924  0.613103 -0.290295  0.087478  0.0  1.0  \n998  0.007711  0.081280  0.016797 -0.068244  0.204852  0.0  1.0  \n999  0.041233  0.121186  0.454881  0.198984  0.442073  1.0  0.0  \n\n[1000 rows x 42 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>32</th>\n      <th>33</th>\n      <th>34</th>\n      <th>35</th>\n      <th>36</th>\n      <th>37</th>\n      <th>38</th>\n      <th>39</th>\n      <th>40</th>\n      <th>41</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.537974</td>\n      <td>0.135781</td>\n      <td>0.156351</td>\n      <td>0.574102</td>\n      <td>0.533220</td>\n      <td>-0.158636</td>\n      <td>-0.101433</td>\n      <td>-0.032645</td>\n      <td>-0.105989</td>\n      <td>-0.063549</td>\n      <td>...</td>\n      <td>0.047417</td>\n      <td>-0.149233</td>\n      <td>-0.432112</td>\n      <td>0.144402</td>\n      <td>0.430720</td>\n      <td>0.462198</td>\n      <td>0.039850</td>\n      <td>0.225659</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.004974</td>\n      <td>0.196055</td>\n      <td>0.178136</td>\n      <td>0.169558</td>\n      <td>0.453874</td>\n      <td>-0.018763</td>\n      <td>-0.040355</td>\n      <td>0.012371</td>\n      <td>-0.003687</td>\n      <td>-0.051264</td>\n      <td>...</td>\n      <td>0.267803</td>\n      <td>-0.022550</td>\n      <td>0.261758</td>\n      <td>0.067600</td>\n      <td>0.342194</td>\n      <td>0.498299</td>\n      <td>-0.174735</td>\n      <td>0.262961</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.498937</td>\n      <td>0.402633</td>\n      <td>0.405978</td>\n      <td>0.595868</td>\n      <td>0.717451</td>\n      <td>-0.152932</td>\n      <td>-0.158300</td>\n      <td>-0.043406</td>\n      <td>-0.252996</td>\n      <td>-0.062804</td>\n      <td>...</td>\n      <td>0.362044</td>\n      <td>0.077862</td>\n      <td>0.012036</td>\n      <td>0.035554</td>\n      <td>0.371593</td>\n      <td>0.464975</td>\n      <td>-0.085389</td>\n      <td>0.151620</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.176377</td>\n      <td>0.066691</td>\n      <td>0.175304</td>\n      <td>0.460799</td>\n      <td>0.465094</td>\n      <td>-0.012672</td>\n      <td>0.098899</td>\n      <td>0.090992</td>\n      <td>0.015092</td>\n      <td>-0.038908</td>\n      <td>...</td>\n      <td>0.044669</td>\n      <td>-0.056171</td>\n      <td>0.157613</td>\n      <td>0.074912</td>\n      <td>-0.193162</td>\n      <td>-0.035965</td>\n      <td>0.159661</td>\n      <td>0.245726</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.088944</td>\n      <td>0.163707</td>\n      <td>0.152545</td>\n      <td>-0.121473</td>\n      <td>0.397098</td>\n      <td>-0.021557</td>\n      <td>0.087762</td>\n      <td>0.077328</td>\n      <td>0.053831</td>\n      <td>0.060876</td>\n      <td>...</td>\n      <td>0.332996</td>\n      <td>-0.046965</td>\n      <td>0.193844</td>\n      <td>-0.001762</td>\n      <td>0.387252</td>\n      <td>0.351752</td>\n      <td>0.175914</td>\n      <td>-0.013901</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>0.648247</td>\n      <td>0.441736</td>\n      <td>0.216079</td>\n      <td>0.641411</td>\n      <td>0.700321</td>\n      <td>0.092613</td>\n      <td>0.160054</td>\n      <td>0.050411</td>\n      <td>0.083747</td>\n      <td>0.334964</td>\n      <td>...</td>\n      <td>0.359591</td>\n      <td>-0.136777</td>\n      <td>0.515093</td>\n      <td>0.154892</td>\n      <td>0.330730</td>\n      <td>0.593833</td>\n      <td>0.009254</td>\n      <td>-0.097751</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>0.087570</td>\n      <td>0.141916</td>\n      <td>0.069388</td>\n      <td>-0.145960</td>\n      <td>0.415810</td>\n      <td>-0.028320</td>\n      <td>-0.122977</td>\n      <td>0.023025</td>\n      <td>-0.050155</td>\n      <td>0.104348</td>\n      <td>...</td>\n      <td>0.269484</td>\n      <td>-0.066122</td>\n      <td>0.412675</td>\n      <td>0.342117</td>\n      <td>-0.363268</td>\n      <td>0.547810</td>\n      <td>0.072129</td>\n      <td>0.068804</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>0.689149</td>\n      <td>0.576419</td>\n      <td>0.431294</td>\n      <td>0.793375</td>\n      <td>0.794358</td>\n      <td>0.135369</td>\n      <td>0.097210</td>\n      <td>0.124003</td>\n      <td>-0.053644</td>\n      <td>-0.115828</td>\n      <td>...</td>\n      <td>0.535664</td>\n      <td>-0.074024</td>\n      <td>0.086301</td>\n      <td>0.294766</td>\n      <td>-0.107924</td>\n      <td>0.613103</td>\n      <td>-0.290295</td>\n      <td>0.087478</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>0.242202</td>\n      <td>0.242578</td>\n      <td>0.245547</td>\n      <td>0.525288</td>\n      <td>0.384736</td>\n      <td>0.007170</td>\n      <td>-0.034059</td>\n      <td>0.000674</td>\n      <td>0.036789</td>\n      <td>0.092761</td>\n      <td>...</td>\n      <td>0.015867</td>\n      <td>-0.076312</td>\n      <td>0.091667</td>\n      <td>0.007711</td>\n      <td>0.081280</td>\n      <td>0.016797</td>\n      <td>-0.068244</td>\n      <td>0.204852</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>-0.367847</td>\n      <td>0.346774</td>\n      <td>0.333824</td>\n      <td>-0.174182</td>\n      <td>0.782559</td>\n      <td>0.017074</td>\n      <td>0.020070</td>\n      <td>0.003414</td>\n      <td>0.056128</td>\n      <td>0.030258</td>\n      <td>...</td>\n      <td>0.297961</td>\n      <td>0.075676</td>\n      <td>0.454723</td>\n      <td>0.041233</td>\n      <td>0.121186</td>\n      <td>0.454881</td>\n      <td>0.198984</td>\n      <td>0.442073</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows Ã— 42 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test1[\"label\"] = test1[[40, 41]].apply(lambda x: x.idxmax() - 40, axis = 1)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:26:14.223617Z","iopub.execute_input":"2023-08-10T15:26:14.223939Z","iopub.status.idle":"2023-08-10T15:26:14.320151Z","shell.execute_reply.started":"2023-08-10T15:26:14.223910Z","shell.execute_reply":"2023-08-10T15:26:14.319235Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"test1.drop(columns = [40, 41], inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:26:14.321116Z","iopub.execute_input":"2023-08-10T15:26:14.321453Z","iopub.status.idle":"2023-08-10T15:26:14.330777Z","shell.execute_reply.started":"2023-08-10T15:26:14.321422Z","shell.execute_reply":"2023-08-10T15:26:14.329773Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"test1","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:26:14.332710Z","iopub.execute_input":"2023-08-10T15:26:14.332971Z","iopub.status.idle":"2023-08-10T15:26:14.363007Z","shell.execute_reply.started":"2023-08-10T15:26:14.332942Z","shell.execute_reply":"2023-08-10T15:26:14.362027Z"},"trusted":true},"execution_count":88,"outputs":[{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"            0         1         2         3         4         5         6  \\\n0    0.537974  0.135781  0.156351  0.574102  0.533220 -0.158636 -0.101433   \n1   -0.004974  0.196055  0.178136  0.169558  0.453874 -0.018763 -0.040355   \n2    0.498937  0.402633  0.405978  0.595868  0.717451 -0.152932 -0.158300   \n3    0.176377  0.066691  0.175304  0.460799  0.465094 -0.012672  0.098899   \n4   -0.088944  0.163707  0.152545 -0.121473  0.397098 -0.021557  0.087762   \n..        ...       ...       ...       ...       ...       ...       ...   \n995  0.648247  0.441736  0.216079  0.641411  0.700321  0.092613  0.160054   \n996  0.087570  0.141916  0.069388 -0.145960  0.415810 -0.028320 -0.122977   \n997  0.689149  0.576419  0.431294  0.793375  0.794358  0.135369  0.097210   \n998  0.242202  0.242578  0.245547  0.525288  0.384736  0.007170 -0.034059   \n999 -0.367847  0.346774  0.333824 -0.174182  0.782559  0.017074  0.020070   \n\n            7         8         9  ...        31        32        33  \\\n0   -0.032645 -0.105989 -0.063549  ...  0.520521  0.047417 -0.149233   \n1    0.012371 -0.003687 -0.051264  ... -0.237409  0.267803 -0.022550   \n2   -0.043406 -0.252996 -0.062804  ... -0.070015  0.362044  0.077862   \n3    0.090992  0.015092 -0.038908  ...  0.456167  0.044669 -0.056171   \n4    0.077328  0.053831  0.060876  ... -0.196098  0.332996 -0.046965   \n..        ...       ...       ...  ...       ...       ...       ...   \n995  0.050411  0.083747  0.334964  ... -0.477747  0.359591 -0.136777   \n996  0.023025 -0.050155  0.104348  ...  0.350236  0.269484 -0.066122   \n997  0.124003 -0.053644 -0.115828  ... -0.135223  0.535664 -0.074024   \n998  0.000674  0.036789  0.092761  ...  0.082615  0.015867 -0.076312   \n999  0.003414  0.056128  0.030258  ...  0.384253  0.297961  0.075676   \n\n           34        35        36        37        38        39  label  \n0   -0.432112  0.144402  0.430720  0.462198  0.039850  0.225659      1  \n1    0.261758  0.067600  0.342194  0.498299 -0.174735  0.262961      0  \n2    0.012036  0.035554  0.371593  0.464975 -0.085389  0.151620      1  \n3    0.157613  0.074912 -0.193162 -0.035965  0.159661  0.245726      1  \n4    0.193844 -0.001762  0.387252  0.351752  0.175914 -0.013901      0  \n..        ...       ...       ...       ...       ...       ...    ...  \n995  0.515093  0.154892  0.330730  0.593833  0.009254 -0.097751      1  \n996  0.412675  0.342117 -0.363268  0.547810  0.072129  0.068804      0  \n997  0.086301  0.294766 -0.107924  0.613103 -0.290295  0.087478      1  \n998  0.091667  0.007711  0.081280  0.016797 -0.068244  0.204852      1  \n999  0.454723  0.041233  0.121186  0.454881  0.198984  0.442073      0  \n\n[1000 rows x 41 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n      <th>34</th>\n      <th>35</th>\n      <th>36</th>\n      <th>37</th>\n      <th>38</th>\n      <th>39</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.537974</td>\n      <td>0.135781</td>\n      <td>0.156351</td>\n      <td>0.574102</td>\n      <td>0.533220</td>\n      <td>-0.158636</td>\n      <td>-0.101433</td>\n      <td>-0.032645</td>\n      <td>-0.105989</td>\n      <td>-0.063549</td>\n      <td>...</td>\n      <td>0.520521</td>\n      <td>0.047417</td>\n      <td>-0.149233</td>\n      <td>-0.432112</td>\n      <td>0.144402</td>\n      <td>0.430720</td>\n      <td>0.462198</td>\n      <td>0.039850</td>\n      <td>0.225659</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.004974</td>\n      <td>0.196055</td>\n      <td>0.178136</td>\n      <td>0.169558</td>\n      <td>0.453874</td>\n      <td>-0.018763</td>\n      <td>-0.040355</td>\n      <td>0.012371</td>\n      <td>-0.003687</td>\n      <td>-0.051264</td>\n      <td>...</td>\n      <td>-0.237409</td>\n      <td>0.267803</td>\n      <td>-0.022550</td>\n      <td>0.261758</td>\n      <td>0.067600</td>\n      <td>0.342194</td>\n      <td>0.498299</td>\n      <td>-0.174735</td>\n      <td>0.262961</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.498937</td>\n      <td>0.402633</td>\n      <td>0.405978</td>\n      <td>0.595868</td>\n      <td>0.717451</td>\n      <td>-0.152932</td>\n      <td>-0.158300</td>\n      <td>-0.043406</td>\n      <td>-0.252996</td>\n      <td>-0.062804</td>\n      <td>...</td>\n      <td>-0.070015</td>\n      <td>0.362044</td>\n      <td>0.077862</td>\n      <td>0.012036</td>\n      <td>0.035554</td>\n      <td>0.371593</td>\n      <td>0.464975</td>\n      <td>-0.085389</td>\n      <td>0.151620</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.176377</td>\n      <td>0.066691</td>\n      <td>0.175304</td>\n      <td>0.460799</td>\n      <td>0.465094</td>\n      <td>-0.012672</td>\n      <td>0.098899</td>\n      <td>0.090992</td>\n      <td>0.015092</td>\n      <td>-0.038908</td>\n      <td>...</td>\n      <td>0.456167</td>\n      <td>0.044669</td>\n      <td>-0.056171</td>\n      <td>0.157613</td>\n      <td>0.074912</td>\n      <td>-0.193162</td>\n      <td>-0.035965</td>\n      <td>0.159661</td>\n      <td>0.245726</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.088944</td>\n      <td>0.163707</td>\n      <td>0.152545</td>\n      <td>-0.121473</td>\n      <td>0.397098</td>\n      <td>-0.021557</td>\n      <td>0.087762</td>\n      <td>0.077328</td>\n      <td>0.053831</td>\n      <td>0.060876</td>\n      <td>...</td>\n      <td>-0.196098</td>\n      <td>0.332996</td>\n      <td>-0.046965</td>\n      <td>0.193844</td>\n      <td>-0.001762</td>\n      <td>0.387252</td>\n      <td>0.351752</td>\n      <td>0.175914</td>\n      <td>-0.013901</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>0.648247</td>\n      <td>0.441736</td>\n      <td>0.216079</td>\n      <td>0.641411</td>\n      <td>0.700321</td>\n      <td>0.092613</td>\n      <td>0.160054</td>\n      <td>0.050411</td>\n      <td>0.083747</td>\n      <td>0.334964</td>\n      <td>...</td>\n      <td>-0.477747</td>\n      <td>0.359591</td>\n      <td>-0.136777</td>\n      <td>0.515093</td>\n      <td>0.154892</td>\n      <td>0.330730</td>\n      <td>0.593833</td>\n      <td>0.009254</td>\n      <td>-0.097751</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>0.087570</td>\n      <td>0.141916</td>\n      <td>0.069388</td>\n      <td>-0.145960</td>\n      <td>0.415810</td>\n      <td>-0.028320</td>\n      <td>-0.122977</td>\n      <td>0.023025</td>\n      <td>-0.050155</td>\n      <td>0.104348</td>\n      <td>...</td>\n      <td>0.350236</td>\n      <td>0.269484</td>\n      <td>-0.066122</td>\n      <td>0.412675</td>\n      <td>0.342117</td>\n      <td>-0.363268</td>\n      <td>0.547810</td>\n      <td>0.072129</td>\n      <td>0.068804</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>0.689149</td>\n      <td>0.576419</td>\n      <td>0.431294</td>\n      <td>0.793375</td>\n      <td>0.794358</td>\n      <td>0.135369</td>\n      <td>0.097210</td>\n      <td>0.124003</td>\n      <td>-0.053644</td>\n      <td>-0.115828</td>\n      <td>...</td>\n      <td>-0.135223</td>\n      <td>0.535664</td>\n      <td>-0.074024</td>\n      <td>0.086301</td>\n      <td>0.294766</td>\n      <td>-0.107924</td>\n      <td>0.613103</td>\n      <td>-0.290295</td>\n      <td>0.087478</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>0.242202</td>\n      <td>0.242578</td>\n      <td>0.245547</td>\n      <td>0.525288</td>\n      <td>0.384736</td>\n      <td>0.007170</td>\n      <td>-0.034059</td>\n      <td>0.000674</td>\n      <td>0.036789</td>\n      <td>0.092761</td>\n      <td>...</td>\n      <td>0.082615</td>\n      <td>0.015867</td>\n      <td>-0.076312</td>\n      <td>0.091667</td>\n      <td>0.007711</td>\n      <td>0.081280</td>\n      <td>0.016797</td>\n      <td>-0.068244</td>\n      <td>0.204852</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>-0.367847</td>\n      <td>0.346774</td>\n      <td>0.333824</td>\n      <td>-0.174182</td>\n      <td>0.782559</td>\n      <td>0.017074</td>\n      <td>0.020070</td>\n      <td>0.003414</td>\n      <td>0.056128</td>\n      <td>0.030258</td>\n      <td>...</td>\n      <td>0.384253</td>\n      <td>0.297961</td>\n      <td>0.075676</td>\n      <td>0.454723</td>\n      <td>0.041233</td>\n      <td>0.121186</td>\n      <td>0.454881</td>\n      <td>0.198984</td>\n      <td>0.442073</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows Ã— 41 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test1.to_csv(\"newSyntheticAdversarial.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:26:14.364574Z","iopub.execute_input":"2023-08-10T15:26:14.364894Z","iopub.status.idle":"2023-08-10T15:26:14.433517Z","shell.execute_reply.started":"2023-08-10T15:26:14.364864Z","shell.execute_reply":"2023-08-10T15:26:14.432413Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"#synthetic_df.to_csv(\"syntheticAttack_UNR-IDD.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:26:14.435140Z","iopub.execute_input":"2023-08-10T15:26:14.435772Z","iopub.status.idle":"2023-08-10T15:26:14.441081Z","shell.execute_reply.started":"2023-08-10T15:26:14.435737Z","shell.execute_reply":"2023-08-10T15:26:14.439277Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import ConfusionMatrixDisplay, classification_report, confusion_matrix\nimport pickle ","metadata":{"execution":{"iopub.status.busy":"2023-08-11T22:53:19.658295Z","iopub.execute_input":"2023-08-11T22:53:19.659054Z","iopub.status.idle":"2023-08-11T22:53:20.154748Z","shell.execute_reply.started":"2023-08-11T22:53:19.659015Z","shell.execute_reply":"2023-08-11T22:53:20.153744Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import sklearn","metadata":{"execution":{"iopub.status.busy":"2023-08-11T22:53:43.257167Z","iopub.execute_input":"2023-08-11T22:53:43.257512Z","iopub.status.idle":"2023-08-11T22:53:43.262039Z","shell.execute_reply.started":"2023-08-11T22:53:43.257484Z","shell.execute_reply":"2023-08-11T22:53:43.260875Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"sklearn.__version__","metadata":{"execution":{"iopub.status.busy":"2023-08-11T22:53:50.608339Z","iopub.execute_input":"2023-08-11T22:53:50.608915Z","iopub.status.idle":"2023-08-11T22:53:50.616928Z","shell.execute_reply.started":"2023-08-11T22:53:50.608880Z","shell.execute_reply":"2023-08-11T22:53:50.615975Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'1.2.2'"},"metadata":{}}]},{"cell_type":"code","source":"target = \"Label\"\ny = new_df[target]\nX = new_data","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:26:14.450982Z","iopub.execute_input":"2023-08-10T15:26:14.451712Z","iopub.status.idle":"2023-08-10T15:26:14.457762Z","shell.execute_reply.started":"2023-08-10T15:26:14.451679Z","shell.execute_reply":"2023-08-10T15:26:14.456815Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"X.columns = X.columns.astype(str)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:26:14.459237Z","iopub.execute_input":"2023-08-10T15:26:14.459870Z","iopub.status.idle":"2023-08-10T15:26:14.466379Z","shell.execute_reply.started":"2023-08-10T15:26:14.459839Z","shell.execute_reply":"2023-08-10T15:26:14.465539Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:26:14.468442Z","iopub.execute_input":"2023-08-10T15:26:14.468773Z","iopub.status.idle":"2023-08-10T15:26:14.481117Z","shell.execute_reply.started":"2023-08-10T15:26:14.468749Z","shell.execute_reply":"2023-08-10T15:26:14.480171Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"params = {\"n_estimators\":[10, 25, 50, 75, 100], \"max_depth\":[5, 10, 15]}\n\nclf = RandomForestClassifier(random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:26:14.484123Z","iopub.execute_input":"2023-08-10T15:26:14.484378Z","iopub.status.idle":"2023-08-10T15:26:14.491318Z","shell.execute_reply.started":"2023-08-10T15:26:14.484355Z","shell.execute_reply":"2023-08-10T15:26:14.490434Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"model_forest = GridSearchCV(\n    clf, \n    param_grid = params,\n    cv = 5, \n    n_jobs = -1\n)\n\nmodel_forest.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:26:14.494210Z","iopub.execute_input":"2023-08-10T15:26:14.495205Z","iopub.status.idle":"2023-08-10T15:26:28.224516Z","shell.execute_reply.started":"2023-08-10T15:26:14.495174Z","shell.execute_reply":"2023-08-10T15:26:28.223431Z"},"trusted":true},"execution_count":96,"outputs":[{"execution_count":96,"output_type":"execute_result","data":{"text/plain":"GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42), n_jobs=-1,\n             param_grid={'max_depth': [5, 10, 15],\n                         'n_estimators': [10, 25, 50, 75, 100]})","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42), n_jobs=-1,\n             param_grid={&#x27;max_depth&#x27;: [5, 10, 15],\n                         &#x27;n_estimators&#x27;: [10, 25, 50, 75, 100]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42), n_jobs=-1,\n             param_grid={&#x27;max_depth&#x27;: [5, 10, 15],\n                         &#x27;n_estimators&#x27;: [10, 25, 50, 75, 100]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","source":"acc_baseline = y_train.value_counts(normalize = True).max()\nacc_baseline","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:26:28.226211Z","iopub.execute_input":"2023-08-10T15:26:28.226629Z","iopub.status.idle":"2023-08-10T15:26:28.235693Z","shell.execute_reply.started":"2023-08-10T15:26:28.226586Z","shell.execute_reply":"2023-08-10T15:26:28.234774Z"},"trusted":true},"execution_count":97,"outputs":[{"execution_count":97,"output_type":"execute_result","data":{"text/plain":"0.5041418157720344"},"metadata":{}}]},{"cell_type":"code","source":"y_pred = model_forest.predict(X_test)\nprint(f\"Training accuracy: {model_forest.score(X_train, y_train)} \\n Test accuracy: {model_forest.score(X_test, y_test)}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:26:28.237033Z","iopub.execute_input":"2023-08-10T15:26:28.238149Z","iopub.status.idle":"2023-08-10T15:26:28.266725Z","shell.execute_reply.started":"2023-08-10T15:26:28.238112Z","shell.execute_reply":"2023-08-10T15:26:28.265864Z"},"trusted":true},"execution_count":98,"outputs":[{"name":"stdout","text":"Training accuracy: 1.0 \n Test accuracy: 1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"y_pred","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:26:28.269778Z","iopub.execute_input":"2023-08-10T15:26:28.270057Z","iopub.status.idle":"2023-08-10T15:26:28.279433Z","shell.execute_reply.started":"2023-08-10T15:26:28.270033Z","shell.execute_reply":"2023-08-10T15:26:28.278528Z"},"trusted":true},"execution_count":99,"outputs":[{"execution_count":99,"output_type":"execute_result","data":{"text/plain":"array([1, 0, 0, ..., 1, 1, 0])"},"metadata":{}}]},{"cell_type":"code","source":"with open(\"model.pkl\", \"wb\") as f:\n    pickle.dump(model_forest, f)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:30:01.552830Z","iopub.execute_input":"2023-08-10T15:30:01.553264Z","iopub.status.idle":"2023-08-10T15:30:01.560443Z","shell.execute_reply.started":"2023-08-10T15:30:01.553230Z","shell.execute_reply":"2023-08-10T15:30:01.559333Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"synthetic_df.columns = X.columns ","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:26:28.288223Z","iopub.execute_input":"2023-08-10T15:26:28.289057Z","iopub.status.idle":"2023-08-10T15:26:28.295792Z","shell.execute_reply.started":"2023-08-10T15:26:28.289021Z","shell.execute_reply":"2023-08-10T15:26:28.294671Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"synthetic_df","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:26:28.297281Z","iopub.execute_input":"2023-08-10T15:26:28.298032Z","iopub.status.idle":"2023-08-10T15:26:28.329111Z","shell.execute_reply.started":"2023-08-10T15:26:28.298000Z","shell.execute_reply":"2023-08-10T15:26:28.328096Z"},"trusted":true},"execution_count":102,"outputs":[{"execution_count":102,"output_type":"execute_result","data":{"text/plain":"     Received Packets  Received Bytes  Sent Bytes  Sent Packets  \\\n0            0.537974        0.135781    0.156351      0.574102   \n1           -0.004974        0.196055    0.178136      0.169558   \n2            0.498937        0.402633    0.405978      0.595868   \n3            0.176377        0.066691    0.175304      0.460799   \n4           -0.088944        0.163707    0.152545     -0.121473   \n..                ...             ...         ...           ...   \n995          0.648247        0.441736    0.216079      0.641411   \n996          0.087570        0.141916    0.069388     -0.145960   \n997          0.689149        0.576419    0.431294      0.793375   \n998          0.242202        0.242578    0.245547      0.525288   \n999         -0.367847        0.346774    0.333824     -0.174182   \n\n     Port alive Duration (S)  Packets Rx Dropped  Packets Tx Dropped  \\\n0                   0.533220           -0.158636           -0.101433   \n1                   0.453874           -0.018763           -0.040355   \n2                   0.717451           -0.152932           -0.158300   \n3                   0.465094           -0.012672            0.098899   \n4                   0.397098           -0.021557            0.087762   \n..                       ...                 ...                 ...   \n995                 0.700321            0.092613            0.160054   \n996                 0.415810           -0.028320           -0.122977   \n997                 0.794358            0.135369            0.097210   \n998                 0.384736            0.007170           -0.034059   \n999                 0.782559            0.017074            0.020070   \n\n     Packets Rx Errors  Packets Tx Errors  Delta Received Packets  ...  \\\n0            -0.032645          -0.105989               -0.063549  ...   \n1             0.012371          -0.003687               -0.051264  ...   \n2            -0.043406          -0.252996               -0.062804  ...   \n3             0.090992           0.015092               -0.038908  ...   \n4             0.077328           0.053831                0.060876  ...   \n..                 ...                ...                     ...  ...   \n995           0.050411           0.083747                0.334964  ...   \n996           0.023025          -0.050155                0.104348  ...   \n997           0.124003          -0.053644               -0.115828  ...   \n998           0.000674           0.036789                0.092761  ...   \n999           0.003414           0.056128                0.030258  ...   \n\n            0         1         2         3         4         5         6  \\\n0   -0.201305  0.520521  0.047417 -0.149233 -0.432112  0.144402  0.430720   \n1   -0.014036 -0.237409  0.267803 -0.022550  0.261758  0.067600  0.342194   \n2   -0.099907 -0.070015  0.362044  0.077862  0.012036  0.035554  0.371593   \n3   -0.003594  0.456167  0.044669 -0.056171  0.157613  0.074912 -0.193162   \n4    0.035177 -0.196098  0.332996 -0.046965  0.193844 -0.001762  0.387252   \n..        ...       ...       ...       ...       ...       ...       ...   \n995 -0.013036 -0.477747  0.359591 -0.136777  0.515093  0.154892  0.330730   \n996 -0.059265  0.350236  0.269484 -0.066122  0.412675  0.342117 -0.363268   \n997 -0.089380 -0.135223  0.535664 -0.074024  0.086301  0.294766 -0.107924   \n998 -0.033035  0.082615  0.015867 -0.076312  0.091667  0.007711  0.081280   \n999  0.053264  0.384253  0.297961  0.075676  0.454723  0.041233  0.121186   \n\n            7         8         9  \n0    0.462198  0.039850  0.225659  \n1    0.498299 -0.174735  0.262961  \n2    0.464975 -0.085389  0.151620  \n3   -0.035965  0.159661  0.245726  \n4    0.351752  0.175914 -0.013901  \n..        ...       ...       ...  \n995  0.593833  0.009254 -0.097751  \n996  0.547810  0.072129  0.068804  \n997  0.613103 -0.290295  0.087478  \n998  0.016797 -0.068244  0.204852  \n999  0.454881  0.198984  0.442073  \n\n[1000 rows x 40 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Received Packets</th>\n      <th>Received Bytes</th>\n      <th>Sent Bytes</th>\n      <th>Sent Packets</th>\n      <th>Port alive Duration (S)</th>\n      <th>Packets Rx Dropped</th>\n      <th>Packets Tx Dropped</th>\n      <th>Packets Rx Errors</th>\n      <th>Packets Tx Errors</th>\n      <th>Delta Received Packets</th>\n      <th>...</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.537974</td>\n      <td>0.135781</td>\n      <td>0.156351</td>\n      <td>0.574102</td>\n      <td>0.533220</td>\n      <td>-0.158636</td>\n      <td>-0.101433</td>\n      <td>-0.032645</td>\n      <td>-0.105989</td>\n      <td>-0.063549</td>\n      <td>...</td>\n      <td>-0.201305</td>\n      <td>0.520521</td>\n      <td>0.047417</td>\n      <td>-0.149233</td>\n      <td>-0.432112</td>\n      <td>0.144402</td>\n      <td>0.430720</td>\n      <td>0.462198</td>\n      <td>0.039850</td>\n      <td>0.225659</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.004974</td>\n      <td>0.196055</td>\n      <td>0.178136</td>\n      <td>0.169558</td>\n      <td>0.453874</td>\n      <td>-0.018763</td>\n      <td>-0.040355</td>\n      <td>0.012371</td>\n      <td>-0.003687</td>\n      <td>-0.051264</td>\n      <td>...</td>\n      <td>-0.014036</td>\n      <td>-0.237409</td>\n      <td>0.267803</td>\n      <td>-0.022550</td>\n      <td>0.261758</td>\n      <td>0.067600</td>\n      <td>0.342194</td>\n      <td>0.498299</td>\n      <td>-0.174735</td>\n      <td>0.262961</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.498937</td>\n      <td>0.402633</td>\n      <td>0.405978</td>\n      <td>0.595868</td>\n      <td>0.717451</td>\n      <td>-0.152932</td>\n      <td>-0.158300</td>\n      <td>-0.043406</td>\n      <td>-0.252996</td>\n      <td>-0.062804</td>\n      <td>...</td>\n      <td>-0.099907</td>\n      <td>-0.070015</td>\n      <td>0.362044</td>\n      <td>0.077862</td>\n      <td>0.012036</td>\n      <td>0.035554</td>\n      <td>0.371593</td>\n      <td>0.464975</td>\n      <td>-0.085389</td>\n      <td>0.151620</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.176377</td>\n      <td>0.066691</td>\n      <td>0.175304</td>\n      <td>0.460799</td>\n      <td>0.465094</td>\n      <td>-0.012672</td>\n      <td>0.098899</td>\n      <td>0.090992</td>\n      <td>0.015092</td>\n      <td>-0.038908</td>\n      <td>...</td>\n      <td>-0.003594</td>\n      <td>0.456167</td>\n      <td>0.044669</td>\n      <td>-0.056171</td>\n      <td>0.157613</td>\n      <td>0.074912</td>\n      <td>-0.193162</td>\n      <td>-0.035965</td>\n      <td>0.159661</td>\n      <td>0.245726</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.088944</td>\n      <td>0.163707</td>\n      <td>0.152545</td>\n      <td>-0.121473</td>\n      <td>0.397098</td>\n      <td>-0.021557</td>\n      <td>0.087762</td>\n      <td>0.077328</td>\n      <td>0.053831</td>\n      <td>0.060876</td>\n      <td>...</td>\n      <td>0.035177</td>\n      <td>-0.196098</td>\n      <td>0.332996</td>\n      <td>-0.046965</td>\n      <td>0.193844</td>\n      <td>-0.001762</td>\n      <td>0.387252</td>\n      <td>0.351752</td>\n      <td>0.175914</td>\n      <td>-0.013901</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>0.648247</td>\n      <td>0.441736</td>\n      <td>0.216079</td>\n      <td>0.641411</td>\n      <td>0.700321</td>\n      <td>0.092613</td>\n      <td>0.160054</td>\n      <td>0.050411</td>\n      <td>0.083747</td>\n      <td>0.334964</td>\n      <td>...</td>\n      <td>-0.013036</td>\n      <td>-0.477747</td>\n      <td>0.359591</td>\n      <td>-0.136777</td>\n      <td>0.515093</td>\n      <td>0.154892</td>\n      <td>0.330730</td>\n      <td>0.593833</td>\n      <td>0.009254</td>\n      <td>-0.097751</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>0.087570</td>\n      <td>0.141916</td>\n      <td>0.069388</td>\n      <td>-0.145960</td>\n      <td>0.415810</td>\n      <td>-0.028320</td>\n      <td>-0.122977</td>\n      <td>0.023025</td>\n      <td>-0.050155</td>\n      <td>0.104348</td>\n      <td>...</td>\n      <td>-0.059265</td>\n      <td>0.350236</td>\n      <td>0.269484</td>\n      <td>-0.066122</td>\n      <td>0.412675</td>\n      <td>0.342117</td>\n      <td>-0.363268</td>\n      <td>0.547810</td>\n      <td>0.072129</td>\n      <td>0.068804</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>0.689149</td>\n      <td>0.576419</td>\n      <td>0.431294</td>\n      <td>0.793375</td>\n      <td>0.794358</td>\n      <td>0.135369</td>\n      <td>0.097210</td>\n      <td>0.124003</td>\n      <td>-0.053644</td>\n      <td>-0.115828</td>\n      <td>...</td>\n      <td>-0.089380</td>\n      <td>-0.135223</td>\n      <td>0.535664</td>\n      <td>-0.074024</td>\n      <td>0.086301</td>\n      <td>0.294766</td>\n      <td>-0.107924</td>\n      <td>0.613103</td>\n      <td>-0.290295</td>\n      <td>0.087478</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>0.242202</td>\n      <td>0.242578</td>\n      <td>0.245547</td>\n      <td>0.525288</td>\n      <td>0.384736</td>\n      <td>0.007170</td>\n      <td>-0.034059</td>\n      <td>0.000674</td>\n      <td>0.036789</td>\n      <td>0.092761</td>\n      <td>...</td>\n      <td>-0.033035</td>\n      <td>0.082615</td>\n      <td>0.015867</td>\n      <td>-0.076312</td>\n      <td>0.091667</td>\n      <td>0.007711</td>\n      <td>0.081280</td>\n      <td>0.016797</td>\n      <td>-0.068244</td>\n      <td>0.204852</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>-0.367847</td>\n      <td>0.346774</td>\n      <td>0.333824</td>\n      <td>-0.174182</td>\n      <td>0.782559</td>\n      <td>0.017074</td>\n      <td>0.020070</td>\n      <td>0.003414</td>\n      <td>0.056128</td>\n      <td>0.030258</td>\n      <td>...</td>\n      <td>0.053264</td>\n      <td>0.384253</td>\n      <td>0.297961</td>\n      <td>0.075676</td>\n      <td>0.454723</td>\n      <td>0.041233</td>\n      <td>0.121186</td>\n      <td>0.454881</td>\n      <td>0.198984</td>\n      <td>0.442073</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows Ã— 40 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"final_result = test1.rename(columns = {test1.columns[i]:X.columns[i] for i in range(X.shape[1])})","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:26:28.330628Z","iopub.execute_input":"2023-08-10T15:26:28.331029Z","iopub.status.idle":"2023-08-10T15:26:28.339286Z","shell.execute_reply.started":"2023-08-10T15:26:28.330998Z","shell.execute_reply":"2023-08-10T15:26:28.338150Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"final_result","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:26:28.340715Z","iopub.execute_input":"2023-08-10T15:26:28.341289Z","iopub.status.idle":"2023-08-10T15:26:28.373175Z","shell.execute_reply.started":"2023-08-10T15:26:28.341256Z","shell.execute_reply":"2023-08-10T15:26:28.372353Z"},"trusted":true},"execution_count":104,"outputs":[{"execution_count":104,"output_type":"execute_result","data":{"text/plain":"     Received Packets  Received Bytes  Sent Bytes  Sent Packets  \\\n0            0.537974        0.135781    0.156351      0.574102   \n1           -0.004974        0.196055    0.178136      0.169558   \n2            0.498937        0.402633    0.405978      0.595868   \n3            0.176377        0.066691    0.175304      0.460799   \n4           -0.088944        0.163707    0.152545     -0.121473   \n..                ...             ...         ...           ...   \n995          0.648247        0.441736    0.216079      0.641411   \n996          0.087570        0.141916    0.069388     -0.145960   \n997          0.689149        0.576419    0.431294      0.793375   \n998          0.242202        0.242578    0.245547      0.525288   \n999         -0.367847        0.346774    0.333824     -0.174182   \n\n     Port alive Duration (S)  Packets Rx Dropped  Packets Tx Dropped  \\\n0                   0.533220           -0.158636           -0.101433   \n1                   0.453874           -0.018763           -0.040355   \n2                   0.717451           -0.152932           -0.158300   \n3                   0.465094           -0.012672            0.098899   \n4                   0.397098           -0.021557            0.087762   \n..                       ...                 ...                 ...   \n995                 0.700321            0.092613            0.160054   \n996                 0.415810           -0.028320           -0.122977   \n997                 0.794358            0.135369            0.097210   \n998                 0.384736            0.007170           -0.034059   \n999                 0.782559            0.017074            0.020070   \n\n     Packets Rx Errors  Packets Tx Errors  Delta Received Packets  ...  \\\n0            -0.032645          -0.105989               -0.063549  ...   \n1             0.012371          -0.003687               -0.051264  ...   \n2            -0.043406          -0.252996               -0.062804  ...   \n3             0.090992           0.015092               -0.038908  ...   \n4             0.077328           0.053831                0.060876  ...   \n..                 ...                ...                     ...  ...   \n995           0.050411           0.083747                0.334964  ...   \n996           0.023025          -0.050155                0.104348  ...   \n997           0.124003          -0.053644               -0.115828  ...   \n998           0.000674           0.036789                0.092761  ...   \n999           0.003414           0.056128                0.030258  ...   \n\n            1         2         3         4         5         6         7  \\\n0    0.520521  0.047417 -0.149233 -0.432112  0.144402  0.430720  0.462198   \n1   -0.237409  0.267803 -0.022550  0.261758  0.067600  0.342194  0.498299   \n2   -0.070015  0.362044  0.077862  0.012036  0.035554  0.371593  0.464975   \n3    0.456167  0.044669 -0.056171  0.157613  0.074912 -0.193162 -0.035965   \n4   -0.196098  0.332996 -0.046965  0.193844 -0.001762  0.387252  0.351752   \n..        ...       ...       ...       ...       ...       ...       ...   \n995 -0.477747  0.359591 -0.136777  0.515093  0.154892  0.330730  0.593833   \n996  0.350236  0.269484 -0.066122  0.412675  0.342117 -0.363268  0.547810   \n997 -0.135223  0.535664 -0.074024  0.086301  0.294766 -0.107924  0.613103   \n998  0.082615  0.015867 -0.076312  0.091667  0.007711  0.081280  0.016797   \n999  0.384253  0.297961  0.075676  0.454723  0.041233  0.121186  0.454881   \n\n            8         9  label  \n0    0.039850  0.225659      1  \n1   -0.174735  0.262961      0  \n2   -0.085389  0.151620      1  \n3    0.159661  0.245726      1  \n4    0.175914 -0.013901      0  \n..        ...       ...    ...  \n995  0.009254 -0.097751      1  \n996  0.072129  0.068804      0  \n997 -0.290295  0.087478      1  \n998 -0.068244  0.204852      1  \n999  0.198984  0.442073      0  \n\n[1000 rows x 41 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Received Packets</th>\n      <th>Received Bytes</th>\n      <th>Sent Bytes</th>\n      <th>Sent Packets</th>\n      <th>Port alive Duration (S)</th>\n      <th>Packets Rx Dropped</th>\n      <th>Packets Tx Dropped</th>\n      <th>Packets Rx Errors</th>\n      <th>Packets Tx Errors</th>\n      <th>Delta Received Packets</th>\n      <th>...</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.537974</td>\n      <td>0.135781</td>\n      <td>0.156351</td>\n      <td>0.574102</td>\n      <td>0.533220</td>\n      <td>-0.158636</td>\n      <td>-0.101433</td>\n      <td>-0.032645</td>\n      <td>-0.105989</td>\n      <td>-0.063549</td>\n      <td>...</td>\n      <td>0.520521</td>\n      <td>0.047417</td>\n      <td>-0.149233</td>\n      <td>-0.432112</td>\n      <td>0.144402</td>\n      <td>0.430720</td>\n      <td>0.462198</td>\n      <td>0.039850</td>\n      <td>0.225659</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.004974</td>\n      <td>0.196055</td>\n      <td>0.178136</td>\n      <td>0.169558</td>\n      <td>0.453874</td>\n      <td>-0.018763</td>\n      <td>-0.040355</td>\n      <td>0.012371</td>\n      <td>-0.003687</td>\n      <td>-0.051264</td>\n      <td>...</td>\n      <td>-0.237409</td>\n      <td>0.267803</td>\n      <td>-0.022550</td>\n      <td>0.261758</td>\n      <td>0.067600</td>\n      <td>0.342194</td>\n      <td>0.498299</td>\n      <td>-0.174735</td>\n      <td>0.262961</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.498937</td>\n      <td>0.402633</td>\n      <td>0.405978</td>\n      <td>0.595868</td>\n      <td>0.717451</td>\n      <td>-0.152932</td>\n      <td>-0.158300</td>\n      <td>-0.043406</td>\n      <td>-0.252996</td>\n      <td>-0.062804</td>\n      <td>...</td>\n      <td>-0.070015</td>\n      <td>0.362044</td>\n      <td>0.077862</td>\n      <td>0.012036</td>\n      <td>0.035554</td>\n      <td>0.371593</td>\n      <td>0.464975</td>\n      <td>-0.085389</td>\n      <td>0.151620</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.176377</td>\n      <td>0.066691</td>\n      <td>0.175304</td>\n      <td>0.460799</td>\n      <td>0.465094</td>\n      <td>-0.012672</td>\n      <td>0.098899</td>\n      <td>0.090992</td>\n      <td>0.015092</td>\n      <td>-0.038908</td>\n      <td>...</td>\n      <td>0.456167</td>\n      <td>0.044669</td>\n      <td>-0.056171</td>\n      <td>0.157613</td>\n      <td>0.074912</td>\n      <td>-0.193162</td>\n      <td>-0.035965</td>\n      <td>0.159661</td>\n      <td>0.245726</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.088944</td>\n      <td>0.163707</td>\n      <td>0.152545</td>\n      <td>-0.121473</td>\n      <td>0.397098</td>\n      <td>-0.021557</td>\n      <td>0.087762</td>\n      <td>0.077328</td>\n      <td>0.053831</td>\n      <td>0.060876</td>\n      <td>...</td>\n      <td>-0.196098</td>\n      <td>0.332996</td>\n      <td>-0.046965</td>\n      <td>0.193844</td>\n      <td>-0.001762</td>\n      <td>0.387252</td>\n      <td>0.351752</td>\n      <td>0.175914</td>\n      <td>-0.013901</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>0.648247</td>\n      <td>0.441736</td>\n      <td>0.216079</td>\n      <td>0.641411</td>\n      <td>0.700321</td>\n      <td>0.092613</td>\n      <td>0.160054</td>\n      <td>0.050411</td>\n      <td>0.083747</td>\n      <td>0.334964</td>\n      <td>...</td>\n      <td>-0.477747</td>\n      <td>0.359591</td>\n      <td>-0.136777</td>\n      <td>0.515093</td>\n      <td>0.154892</td>\n      <td>0.330730</td>\n      <td>0.593833</td>\n      <td>0.009254</td>\n      <td>-0.097751</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>0.087570</td>\n      <td>0.141916</td>\n      <td>0.069388</td>\n      <td>-0.145960</td>\n      <td>0.415810</td>\n      <td>-0.028320</td>\n      <td>-0.122977</td>\n      <td>0.023025</td>\n      <td>-0.050155</td>\n      <td>0.104348</td>\n      <td>...</td>\n      <td>0.350236</td>\n      <td>0.269484</td>\n      <td>-0.066122</td>\n      <td>0.412675</td>\n      <td>0.342117</td>\n      <td>-0.363268</td>\n      <td>0.547810</td>\n      <td>0.072129</td>\n      <td>0.068804</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>0.689149</td>\n      <td>0.576419</td>\n      <td>0.431294</td>\n      <td>0.793375</td>\n      <td>0.794358</td>\n      <td>0.135369</td>\n      <td>0.097210</td>\n      <td>0.124003</td>\n      <td>-0.053644</td>\n      <td>-0.115828</td>\n      <td>...</td>\n      <td>-0.135223</td>\n      <td>0.535664</td>\n      <td>-0.074024</td>\n      <td>0.086301</td>\n      <td>0.294766</td>\n      <td>-0.107924</td>\n      <td>0.613103</td>\n      <td>-0.290295</td>\n      <td>0.087478</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>0.242202</td>\n      <td>0.242578</td>\n      <td>0.245547</td>\n      <td>0.525288</td>\n      <td>0.384736</td>\n      <td>0.007170</td>\n      <td>-0.034059</td>\n      <td>0.000674</td>\n      <td>0.036789</td>\n      <td>0.092761</td>\n      <td>...</td>\n      <td>0.082615</td>\n      <td>0.015867</td>\n      <td>-0.076312</td>\n      <td>0.091667</td>\n      <td>0.007711</td>\n      <td>0.081280</td>\n      <td>0.016797</td>\n      <td>-0.068244</td>\n      <td>0.204852</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>-0.367847</td>\n      <td>0.346774</td>\n      <td>0.333824</td>\n      <td>-0.174182</td>\n      <td>0.782559</td>\n      <td>0.017074</td>\n      <td>0.020070</td>\n      <td>0.003414</td>\n      <td>0.056128</td>\n      <td>0.030258</td>\n      <td>...</td>\n      <td>0.384253</td>\n      <td>0.297961</td>\n      <td>0.075676</td>\n      <td>0.454723</td>\n      <td>0.041233</td>\n      <td>0.121186</td>\n      <td>0.454881</td>\n      <td>0.198984</td>\n      <td>0.442073</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows Ã— 41 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"final_result.to_csv(\"newSyntheticAdversarial.csv\",index = False)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:26:28.374366Z","iopub.execute_input":"2023-08-10T15:26:28.374878Z","iopub.status.idle":"2023-08-10T15:26:28.441893Z","shell.execute_reply.started":"2023-08-10T15:26:28.374847Z","shell.execute_reply":"2023-08-10T15:26:28.440959Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"predictions = model_forest.predict(synthetic_df)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:26:28.443528Z","iopub.execute_input":"2023-08-10T15:26:28.443888Z","iopub.status.idle":"2023-08-10T15:26:28.454543Z","shell.execute_reply.started":"2023-08-10T15:26:28.443855Z","shell.execute_reply":"2023-08-10T15:26:28.453349Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.neighbors import NearestNeighbors\n\n# Assuming real_data and synthetic_data are numpy arrays\n# real_data shape: (num_real_examples, num_features)\n# synthetic_data shape: (num_synthetic_examples, num_features)\n# real_labels shape: (num_real_examples,)\n\n# Fit nearest neighbors model on real data\nnbrs = NearestNeighbors(n_neighbors=1).fit(X)\n\n# Find indices of closest real examples to synthetic examples\ndistances, indices = nbrs.kneighbors(synthetic_df)\n\n# Assign \"true\" labels to synthetic examples based on closest real examples\nsynthetic_true_labels = y[indices.flatten()]\n\n# Now, synthetic_true_labels is a numpy array of shape (num_synthetic_examples, 1)\n# containing the \"true\" labels for the synthetic examples.\n","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:26:28.455996Z","iopub.execute_input":"2023-08-10T15:26:28.456748Z","iopub.status.idle":"2023-08-10T15:26:28.608361Z","shell.execute_reply.started":"2023-08-10T15:26:28.456712Z","shell.execute_reply":"2023-08-10T15:26:28.607042Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(synthetic_true_labels, predictions)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:26:28.609765Z","iopub.execute_input":"2023-08-10T15:26:28.610299Z","iopub.status.idle":"2023-08-10T15:26:28.621143Z","shell.execute_reply.started":"2023-08-10T15:26:28.610266Z","shell.execute_reply":"2023-08-10T15:26:28.619790Z"},"trusted":true},"execution_count":108,"outputs":[{"execution_count":108,"output_type":"execute_result","data":{"text/plain":"array([[454,  32],\n       [  4, 510]])"},"metadata":{}}]},{"cell_type":"code","source":"ConfusionMatrixDisplay.from_estimator(model_forest, synthetic_df, synthetic_true_labels)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:26:28.631030Z","iopub.execute_input":"2023-08-10T15:26:28.631711Z","iopub.status.idle":"2023-08-10T15:26:28.936460Z","shell.execute_reply.started":"2023-08-10T15:26:28.631678Z","shell.execute_reply":"2023-08-10T15:26:28.935422Z"},"trusted":true},"execution_count":109,"outputs":[{"execution_count":109,"output_type":"execute_result","data":{"text/plain":"<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7c8066a0b280>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 2 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"synthetic_true_labels.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:26:28.937800Z","iopub.execute_input":"2023-08-10T15:26:28.938427Z","iopub.status.idle":"2023-08-10T15:26:28.946117Z","shell.execute_reply.started":"2023-08-10T15:26:28.938390Z","shell.execute_reply":"2023-08-10T15:26:28.945158Z"},"trusted":true},"execution_count":110,"outputs":[{"execution_count":110,"output_type":"execute_result","data":{"text/plain":"1    514\n0    486\nName: Label, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"new_df[\"Label\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:26:28.947851Z","iopub.execute_input":"2023-08-10T15:26:28.948529Z","iopub.status.idle":"2023-08-10T15:26:28.957434Z","shell.execute_reply.started":"2023-08-10T15:26:28.948478Z","shell.execute_reply":"2023-08-10T15:26:28.956552Z"},"trusted":true},"execution_count":111,"outputs":[{"execution_count":111,"output_type":"execute_result","data":{"text/plain":"0    3773\n1    3773\nName: Label, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}